---
title: 'Base Line: Time Series'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# packages
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(forecast))
```

## Introduction

To start with a base line model, we analyze the number of exceptions for each week. We use time series to make predictions for the next 53 weeks based on given data.

## Prepare Data

`train.csv` is the training set, generated by `split_by_year.R`. including data till end of 2017.

```{r load data}
df <- read_csv("../data/train.csv")
```

For the base line model, we count the number of exceptions for each week. We use `week()` from `lubridate` package, which is not the actual week but every 7 day starts from the beginning of the year. In this way, a year has 53 weeks, that the last week has only one day, which is Dec 31st.

We will exclude the data for Dec 31st, 2012 for now. So the data is from Jan 1st 2013 to Dec 31st 2017.

```{r count by week}
weekly <- df %>% 
  # exclude data for 2012-12-31 for now
  filter(year(SHIFT_DATE) > 2012) %>% 
  # extract year and week
  mutate(year = year(SHIFT_DATE),
         week = week(SHIFT_DATE)) %>% 
  # get number of weekly exceptions
  group_by(year, week) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  # index column for plotting
  mutate(id = seq.int(265))

head(weekly)
```

For model adjusting, we set the data in 2017 as the validation set.

```{r split train and validation}
# split train and validation set
weekly_train <- weekly %>% 
  filter(year < 2017)
weekly_val <- weekly %>%
  filter(year == 2017)
```

## Time Series

We use loess method with log transformed data to make predictions.

> Reference: https://www.datascience.com/blog/decomposition-based-approaches-to-time-series-forecasting

```{r}
# create time series
ts_week_train <- ts(weekly_train$count, start = c(2013,1), frequency = 53)

# fit
stl_week <- stlf(log10(ts_week_train), s.window = "periodic", method = 'arima')

# predict
fc <- forecast(stl_week, method = "arima", h = 53)

# add columns
week_stl_train <- weekly_train %>% 
  mutate(loess = 10^(stl_week$fitted))
week_stl_val <- weekly_val %>% 
  mutate(loess = 10^(fc$mean))

# make plot
ggplot() +
  # original data
  geom_line(data = week_stl_train, aes(x = id, y = count, color = "Train")) +
  geom_line(data = week_stl_val, aes(x = id, y = count, color = "Validation")) +
  # predictions
  geom_line(data = week_stl_train, aes(x = id, y = loess, color = "Prediction")) +
  geom_line(data = week_stl_val, aes(x = id, y = loess, color = "Prediction")) +
  # labs
  labs(x = "Week", y = "Number of Exceptions Per Week",
       title = "Time Series: Loess with log transformed data") +
  scale_color_discrete(name = "Models") +
  theme(plot.title = element_text(hjust = 0.5))
```

## Accuracy Measure

We use Mean Absolute Percentage Error(MAPE) to mesure the accuracy of this model. (Check the [wiki](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error) for more details.)

```{r}
# MAPE
(sum(abs((week_stl_val$count - week_stl_val$loess)/week_stl_val$count))/nrow(week_stl_val))
```

According to the result, the MAPE is approximately `8.62%`. We can use this or other measurements to compare with other approaches.
