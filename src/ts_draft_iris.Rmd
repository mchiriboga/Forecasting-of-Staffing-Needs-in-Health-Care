---
title: "Time Series Draft"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# packages
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(forecast))
```

## Readme

This is a draft by Iris, exploring time series of the data. Includes unfinished pieces of work, just for reference.

**PLEASE DO NOT KNIT THE WHOLE FILE.**

## Load Data

`train.csv` is the training set, including data till end of 2017.

Generated by `split_by_year.R`.

```{r load data, include=FALSE}
df <- read_csv("../data/train.csv")
```

## Daily Analysis (TBC)

- Data in 2012 has only the day `2012-12-31`.
- Remove the leap year `2016-02-29` to use `frequency = 365`.
- Total number of rows after grouping: (365*5)+1=1826.

```{r daily: data wrangling}
# daily number of exceptions from 2012-12-31 to 2017-12-31
daily <- df %>%
  # remove extra day in leap year 2016
  filter(SHIFT_DATE != as.Date("2016-02-29")) %>% 
  # summarise the number of exceptions in a day
  group_by(SHIFT_DATE) %>% 
  summarise(count = n()) %>% 
  ungroup()

# check number of entries
nrow(daily)
```

```{r daily: time series}
# create time series for daily data
ts_day <- ts(daily$count, frequency = 365)
# decompose
dec_day <- decompose(ts_day)
# visualize
plot(dec_day)
# plot acf
acf(ts_day)
```

## Weekly Analysis

- Aggregate weekly data throughout the year.
- Note: lubridate `week()` does not gives correct week; just every 7 days each year.

```{r weekly: data wrangling}
weekly <- df %>% 
  # exclude data for 2012-12-31 for now
  filter(year(SHIFT_DATE) > 2012) %>% 
  # extract year and week
  mutate(year = year(SHIFT_DATE),
         week = week(SHIFT_DATE)) %>% 
  # get number of weekly exceptions
  group_by(year, week) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  # index column for plotting
  mutate(id = seq.int(265))

head(weekly)
```

```{r weekly: test cell, eval = FALSE}
# explore the week pattern
# DO NOT render now

temp <- df %>% 
  filter(year(SHIFT_DATE) > 2012) %>% 
  mutate(year = year(SHIFT_DATE),
         week = week(SHIFT_DATE)) %>% 
  filter(week == 53 | week == 1) %>% 
  group_by(SHIFT_DATE, week) %>% 
  summarise(count = n())

week(as.Date("2013-01-08"))
```

**Cheatsheet:**

- `decompose`: `dec$seasonal`, `dec$trend`
- Holt-Winter with more frequency: use `tbat`

```{r weekly: time series}
# split train and validation set
weekly_train <- weekly %>% 
  filter(year < 2017)
weekly_val <- weekly %>%
  filter(year == 2017)

# create time series
ts_week_train <- ts(weekly_train$count, start = c(2013,1), frequency = 53)
#ts_week_val <- ts(weekly_val$count, start = c(2017,1), frequency = 53)

# loess method
stl_week <- stl(ts_week_train, s.window = "periodic")

# forecast
fc <- forecast(stl_week, method = "arima", h = 53)
plot(fc$mean)

# fields of loess
#stl_week$time.series[,'seasonal']  # seasonality
#stl_week$time.series[,2]  # trend

week_stl_train <- weekly_train %>% 
  # holt-winter
  #mutate(hw_pred = tbats(ts_week)$fitted) %>% 
  # loess method
  mutate(loess = stl_week$time.series[,'seasonal']) %>% 
  mutate(loess = loess + stl_week$time.series[,2])

week_stl_val <- weekly_val %>% 
  mutate(loess = fc$mean)

# make base plot
stl_base <- ggplot() +
  # train data
  geom_line(data = week_stl_train, aes(x = id, y = count)) +
  # validation prediction
  geom_line(data = week_stl_val, aes(x = id, y = count), color = 'blue')
  
# add prediction
stl_base +
  geom_line(data = week_stl_train, aes(x = id, y = loess), color = 'red') +
  geom_line(data = week_stl_val, aes(x = id, y = loess), color = 'red')
```

```{r}
# Try other methods
# STL with log transformed data
# Ref: https://www.datascience.com/blog/decomposition-based-approaches-to-time-series-forecasting

# fit
stl_week <- stlf(log10(ts_week_train), s.window = "periodic", method = 'arima')

# predict
fc <- forecast(stl_week, method = "arima", h = 53)

# add columns
week_stl_train <- weekly_train %>% 
  mutate(loess = 10^(stl_week$fitted))
week_stl_val <- weekly_val %>% 
  mutate(loess = 10^(fc$mean))

# add prediction
stl_base +
  geom_line(data = week_stl_train, aes(x = id, y = loess), color = 'red') +
  geom_line(data = week_stl_val, aes(x = id, y = loess), color = 'red')
```


--------

**Unfinished Analysis Below**

--------

## All Years, number of exceptions

- Group by day
- Use data 2012 to 2017

```{r number of exceptions by day}
all_num_day <- df %>% 
  filter(SHIFT_DATE < as.Date("2018-01-01")) %>% 
  group_by(SHIFT_DATE) %>% 
  summarise(count = n()) %>% 
  ungroup()
```

```{r}
tsr <- ts(all_num_day$count, frequency = 7)
plot(tsr)
```

## Week/Day analysis

```{r}
df18 %>% 
  mutate(week = week(SHIFT_DATE), weekday = wday(SHIFT_DATE)) %>% 
  group_by(SHIFT_DATE, week, weekday) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  ggplot(aes(x = weekday, y = count, color = as.factor(week))) +
  geom_line()
```

```{r}
test <- df18 %>% 
  group_by(SHIFT_DATE) %>% 
  summarise(count = n())

tsrr <- ts(test$count, frequency = 7)
plot(tsrr)
dec <- decompose(tsrr)

dec$seasonal
```

## Hours of exceptions

```{r}
df_hours <- df %>% 
  # generate shift time to hours
  mutate(hours = as.numeric((END_TIME - START_TIME)/3600)) %>% 
  # for the shift time across midnight
  # adjust the negative result to actual hours 
  mutate(hours = ifelse(hours > 0, hours, 24 + hours))
```